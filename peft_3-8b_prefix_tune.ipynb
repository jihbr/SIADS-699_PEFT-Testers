{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7858698e-0108-4d78-81e7-d4fa61357e7b",
   "metadata": {},
   "source": [
    "# Prefix Tuning Full USMLE on Llama 3-8b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217919ca-3bad-47f1-8dad-0edeab705277",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9b4e60-645c-4116-b38a-d1c7a33329b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.6\n",
      "VRAM: 23.57GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    default_data_collator, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model, \n",
    "    PrefixTuningConfig, \n",
    "    TaskType\n",
    ")\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#  GPU Verification\n",
    "assert torch.cuda.is_available(), \"GPU not detected!\"\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- 3. Model and Tokenizer Setup ---\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # For causal LM padding\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# Adjust hyperparameters for full dataset and larger model\n",
    "max_length = 512  # Increased for longer sequences\n",
    "lr = 5e-3  # Slightly lower learning rate for stability\n",
    "num_epochs = 1  # Reduced epochs for full dataset\n",
    "batch_size = 1  # Reduced batch size due to larger model\n",
    "gradient_accumulation_steps = 8  # Accumulate gradients to simulate larger batch\n",
    "eval_steps = 500  # Evaluate every 500 steps\n",
    "save_steps = 1000  # Save checkpoints every 1000 steps\n",
    "logging_steps = 50  # Log more frequently\n",
    "max_grad_norm = 1.0  # Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ed1f2-2b5e-4660-9b28-1f0e134b0211",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06aaa543-725d-4595-be0e-720a13be6e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "    num_rows: 10178\n",
      "})\n",
      "Full dataset size: 10178\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..'))) \n",
    "from src.helper_functions import format_mcf_finetuning\n",
    "\n",
    "# Use full dataset instead of sample\n",
    "usml_raw = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "usml_train = usml_raw['train']\n",
    "# Remove the sampling - use full dataset\n",
    "full_train = usml_train\n",
    "print(full_train)\n",
    "print(f\"Full dataset size: {len(full_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4503c95e-fd11-4e66-8109-bb5c04357328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 9669\n",
      "Validation samples: 509\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split for monitoring\n",
    "train_size = int(0.95 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_dataset = full_train.select(range(train_size))\n",
    "val_dataset = full_train.select(range(train_size, train_size + val_size))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "## Format Data\n",
    "formatted_train = train_dataset.map(\n",
    "    format_mcf_finetuning,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4  # Use multiprocessing for faster processing\n",
    ")\n",
    "\n",
    "formatted_val = val_dataset.map(\n",
    "    format_mcf_finetuning,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e0d08-e639-4afe-a360-3268dda1f5ad",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2877884c-3cf1-45e3-86f9-7cca5c9dac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "Tokenizing validation data...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    texts = [p + c for p, c in zip(examples['prompt'], examples['completion'])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels (mask prompt tokens)\n",
    "    prompt_lens = [len(tokenizer(p)['input_ids']) for p in examples['prompt']]\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i, plen in enumerate(prompt_lens):\n",
    "        labels[i, :plen] = -100\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Process full dataset with multiprocessing\n",
    "print(\"Tokenizing training data...\")\n",
    "tokenized_train = formatted_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion'],\n",
    "    batch_size=16,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation data...\")\n",
    "tokenized_val = formatted_val.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion'],\n",
    "    batch_size=16,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570eb7d-e1fe-47ad-8bc7-e10832cf9bae",
   "metadata": {},
   "source": [
    "## Prefix Tuning Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06616f4-c5ed-487d-bb7d-10e4ee40676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama 3 8b model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e869aedbd94ad2ba34bd5d57e6ae76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 8,032,227,328 || trainable%: 0.0245\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Adjust prefix tuning config for Llama 3\n",
    "peft_config = PrefixTuningConfig(\n",
    "    peft_type=\"PREFIX_TUNING\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=30,\n",
    "    prefix_projection=False,             # Keep this if you want projection\n",
    "    # token_dim=8192,                     # Must match LLaMA 3 8B's hidden size\n",
    "    # encoder_hidden_size=512,           # This is the intermediate size for projection (can be tuned)\n",
    ")\n",
    "    \n",
    "# Load Llama 3 8b with optimized settings\n",
    "print(\"Loading Llama 3 8b model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=True,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # Disable cache for training\n",
    "    # attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # Enable gradient checkpointing for memory efficiency\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3b6c0-6584-4215-84d7-36b88536ae01",
   "metadata": {},
   "source": [
    "## Training Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb68ed18-2061-4b2a-af9e-b069b26d9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-8b-prefix-full-v2\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=False,  # Change this to False\n",
    "    # Remove these lines since we're not loading best model:\n",
    "    # metric_for_best_model=\"eval_loss\",\n",
    "    # greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    push_to_hub=False,\n",
    "    # Remove hub settings for now\n",
    "    # hub_model_id=\"pippalap/llama3-8b-prefix-full\",\n",
    "    # hub_strategy=\"checkpoint\",\n",
    "    # Memory optimization\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    # Checkpointing\n",
    "    save_total_limit=3,  # Keep only 3 checkpoints\n",
    "    # Remove resume_from_checkpoint for initial run\n",
    "    # resume_from_checkpoint=True,\n",
    "    # Fix label names issue\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # Optimize for tensor cores\n",
    ")\n",
    "\n",
    "# # Optimizer with better settings for larger models\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     model.parameters(), \n",
    "#     lr=lr,\n",
    "#     betas=(0.9, 0.95),  # Better betas for large models\n",
    "#     eps=1e-8,\n",
    "#     weight_decay=0.01\n",
    "# )\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# total_steps = len(tokenized_train) // (batch_size * gradient_accumulation_steps) * num_epochs\n",
    "# lr_scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=100,\n",
    "#     num_training_steps=total_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6889a-e24b-4856-be77-10c8734a2e1e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43a9445-b3d2-4e43-9f90-b2f1992c4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint-1000...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1208' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1208/1209 16:21 < 00:04, 0.21 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1208, training_loss=0.27535978058316063, metrics={'train_runtime': 986.9604, 'train_samples_per_second': 9.797, 'train_steps_per_second': 1.225, 'total_flos': 2.229200383597609e+17, 'train_loss': 0.27535978058316063, 'epoch': 0.9994828834419278})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,  # Add validation dataset\n",
    "    data_collator=data_collator,\n",
    "    # optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "print(\"Resuming training from checkpoint-1000...\")\n",
    "trainer.train(resume_from_checkpoint=\"./llama3-8b-prefix-full-v2/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1636d-401d-4d81-bf5a-69d09e012889",
   "metadata": {},
   "source": [
    "## Save adapters - to not exclude full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40f07ac-a457-4641-a8d3-30b0d4ddd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    \"llama8b-usmle-prefix-tune\",\n",
    "    safe_serialization=True,  # Uses modern .safetensors format\n",
    "    max_shard_size=\"200MB\"  # Optional: splits large adapters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f1f5d-c8e9-4159-8763-640fe5c572e3",
   "metadata": {},
   "source": [
    "## Upload Model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4726c045-98d7-4ea1-abb1-326c6a8ceaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.33.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.53.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d24a6ac0-808b-4dec-bc40-fb6956a5d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64ca678-e5c5-442b-831a-a7b48a3b4c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:1221: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-68752c70-066969793e3f2a74122c8e90;926ff71c-56cb-4bfd-95bf-608275dc0c3c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Meta-Llama-3-8B has been rejected by the repo's authors. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:238: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f2158f63cb4c8f9215ee49d161a573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/7.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pippalap/llama8b-usmle-prefix-tune/commit/91ddc2fae385b2f305912b3f0b8174faab3508a7', commit_message='Upload model', commit_description='', oid='91ddc2fae385b2f305912b3f0b8174faab3508a7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pippalap/llama8b-usmle-prefix-tune', endpoint='https://huggingface.co', repo_type='model', repo_id='pippalap/llama8b-usmle-prefix-tune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define your custom model name\n",
    "MODEL_NAME = \"llama8b-usmle-prefix-tune\"  \n",
    "USERNAME = \"pippalap\"  # Your Hugging Face username\n",
    "\n",
    "model.push_to_hub(\"pippalap/llama8b-usmle-prefix-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e2af6-ae82-4d33-a62b-73c1f8a2bbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b22846c-5521-4fc9-9492-ed169d6e2d57",
   "metadata": {},
   "source": [
    "## *Extra* Verifying configurations and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a06b2830-76b9-4a0f-8283-ea887cc82bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check checkpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005e4417-c845-41fa-a9fb-bf13dd4787c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PEFT config:\n",
      "{\n",
      "  \"auto_mapping\": null,\n",
      "  \"base_model_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"encoder_hidden_size\": 1024,\n",
      "  \"inference_mode\": true,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_layers\": 32,\n",
      "  \"num_transformer_submodules\": 1,\n",
      "  \"num_virtual_tokens\": 30,\n",
      "  \"peft_type\": \"PREFIX_TUNING\",\n",
      "  \"prefix_projection\": true,\n",
      "  \"revision\": null,\n",
      "  \"task_type\": \"CAUSAL_LM\",\n",
      "  \"token_dim\": 1024\n",
      "}\n",
      "\n",
      "Files in checkpoint:\n",
      "['adapter_model.safetensors', '.ipynb_checkpoints', 'optimizer.pt', 'scheduler.pt', 'rng_state.pth', 'training_args.bin', 'adapter_config.json', 'tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json', 'trainer_state.json', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"./llama3-8b-prefix-full/checkpoint-1000\"\n",
    "\n",
    "# Check the saved adapter config\n",
    "config_path = os.path.join(checkpoint_path, \"adapter_config.json\")\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        saved_config = json.load(f)\n",
    "    print(\"Saved PEFT config:\")\n",
    "    print(json.dumps(saved_config, indent=2))\n",
    "else:\n",
    "    print(\"No adapter_config.json found\")\n",
    "\n",
    "# List all files in checkpoint\n",
    "print(\"\\nFiles in checkpoint:\")\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "# Check if there's a saved adapter weights file\n",
    "adapter_weights_path = os.path.join(checkpoint_path, \"adapter_model.bin\")\n",
    "if os.path.exists(adapter_weights_path):\n",
    "    import torch\n",
    "    weights = torch.load(adapter_weights_path, map_location='cpu')\n",
    "    print(f\"\\nSaved adapter weights keys: {list(weights.keys())}\")\n",
    "    if 'prompt_embeddings' in weights:\n",
    "        print(f\"Prompt embeddings shape: {weights['prompt_embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc0b9ffa-2d84-4a51-91b8-0379f06a2263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt_embeddings'])\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "adapter_weights = load_file(\"./llama3-8b-prefix-full/checkpoint-1000/adapter_model.safetensors\")\n",
    "\n",
    "# Show all available keys\n",
    "print(adapter_weights.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d975d65-3ddb-4bd1-b3ba-de7f74327a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 65536])\n"
     ]
    }
   ],
   "source": [
    "print(adapter_weights['prompt_embeddings'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aca25-7f97-46f5-b922-c39798e9b817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
