{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA PEFT Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "    num_rows: 10178\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..'))) \n",
    "from src.helper_functions import format_mcf_finetuning\n",
    "\n",
    "\n",
    "# Load model\n",
    "usml_raw = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "usml_train = usml_raw['train']\n",
    "print(usml_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10178/10178 [00:00<00:00, 26938.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "A. Ampicillin\n",
      "B. Ceftriaxone\n",
      "C. Doxycycline\n",
      "D. Nitrofurantoin\n",
      "Answer:\n",
      "Nitrofurantoin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_train = usml_train.map(\n",
    "    format_mcf_finetuning,\n",
    "    remove_columns=usml_train.column_names\n",
    ")\n",
    "\n",
    "print(formatted_train[0]['prompt'])\n",
    "print(formatted_train[0]['completion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3100, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3155, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3367, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3612, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3672, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rf/qbnw5dmn5dq1gms0pls5p4b40000gn/T/ipykernel_55186/1102234074.py\", line 2, in <module>\n",
      "    from transformers import (\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/transformers/trainer.py\", line 77, in <module>\n",
      "    from .trainer_pt_utils import (\n",
      "  File \"/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/transformers/trainer_pt_utils.py\", line 211, in <module>\n",
      "    device: Optional[torch.device] = torch.device(\"cuda\"),\n",
      "/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:211: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n",
      "/Users/joshhaber/SIADS-699_PEFT-Testers/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "GPU not detected!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 1. GPU Verification\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.cuda.is_available(), \u001b[33m\"\u001b[39m\u001b[33mGPU not detected!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCUDA version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.version.cuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVRAM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.get_device_properties(\u001b[32m0\u001b[39m).total_memory/\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: GPU not detected!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 1. GPU Verification\n",
    "assert torch.cuda.is_available(), \"GPU not detected!\"\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Tokenizer Setup\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 3. Tokenization Function\n",
    "def tokenize_function(examples):\n",
    "    texts = [p + c for p, c in zip(examples['prompt'], examples['completion'])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels (mask prompt tokens)\n",
    "    prompt_lens = [len(tokenizer(p)['input_ids']) for p in examples['prompt']]\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i, plen in enumerate(prompt_lens):\n",
    "        labels[i, :plen] = -100\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# 4. Apply tokenization\n",
    "tokenized_dataset = formatted_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion'],\n",
    "    batch_size=8  # Smaller batches for tokenization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model, Configure LoRA, Collate Data, and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Loading (FP16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=True\n",
    ")\n",
    "\n",
    "# 6. LoRA Configuration (Standard, no quantization prep)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Reduced from 16 for memory\n",
    "    lora_alpha=16, #alpha typically 2x rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Full attention layer\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 7. Verify parameters\n",
    "model.print_trainable_parameters()  # Should show ~0.1% of parameters\n",
    "\n",
    "# 8. Training Arguments (Memory-optimized)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama7b-usmle-lora\",\n",
    "    per_device_train_batch_size=2,  # Reduced from 4\n",
    "    gradient_accumulation_steps=8,  # Increased to compensate\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 9. Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# 10. Pre-Training Memory Check\n",
    "print(f\"Pre-train VRAM: {torch.cuda.memory_allocated()/1e9:.2f}GB used\")\n",
    "\n",
    "# 11. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Save adapters (PEFT automatically excludes base model)\n",
    "model.save_pretrained(\n",
    "    \"llama7b-usmle-lora\",\n",
    "    safe_serialization=True,  # Uses modern .safetensors format\n",
    "    max_shard_size=\"200MB\"  # Optional: splits large adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your custom model name\n",
    "MODEL_NAME = \"usmle-llama7b-lora\"  \n",
    "USERNAME = \"jihbr\"  # Your Hugging Face username\n",
    "\n",
    "# 2. Verify critical files\n",
    "import os\n",
    "required_files = {\n",
    "    \"adapter_config.json\": \"LoRA configuration\",\n",
    "    \"adapter_model.safetensors\": \"Adapter weights\"\n",
    "}\n",
    "for file, desc in required_files.items():\n",
    "    assert os.path.exists(f\"llama7b-usmle-lora-subset/{file}\"), f\"Missing {desc} file\"\n",
    "\n",
    "# 3. Accelerated upload (5x faster in GCP)\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# 4. Upload with custom name\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Create the repository with your custom name\n",
    "repo_url = create_repo(\n",
    "    repo_id=f\"{USERNAME}/{MODEL_NAME}\",  # Changed here\n",
    "    repo_type=\"model\",\n",
    "    token=True,\n",
    "    exist_ok=True\n",
    ")\n",
    "print(f\"Created repository: {repo_url}\")\n",
    "\n",
    "# Upload to your named repository\n",
    "HfApi().upload_folder(\n",
    "    folder_path=\"llama7b-usmle-lora\",\n",
    "    repo_id=f\"{USERNAME}/{MODEL_NAME}\",  # Changed here\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Add USMLE LoRA adapters - Medical QA Fine-Tune\",\n",
    "    ignore_patterns=[\"*.bin\", \"pytorch_model*\"],\n",
    "    allow_patterns=[\"adapter_*\", \"README*\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Adapters uploaded successfully to:\")\n",
    "print(f\"https://huggingface.co/{USERNAME}/{MODEL_NAME}\")  # Changed here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
