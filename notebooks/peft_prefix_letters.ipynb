{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86961b21-a2e6-4196-b9a0-d99c40ecd376",
   "metadata": {},
   "source": [
    "# Prefix Tuning Full USMLE on Letter Answer Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c3369-223e-4806-9625-53d103c34321",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c9bdb8-0651-4a94-bb98-2247f8afa097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.6\n",
      "VRAM: 23.57GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    default_data_collator, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model, \n",
    "    PrefixTuningConfig, \n",
    "    TaskType\n",
    ")\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- GPU Verification ---\n",
    "assert torch.cuda.is_available(), \"GPU not detected!\"\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- Model and Tokenizer Setup ---\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # For causal LM padding\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# Adjust hyperparameters for full dataset and larger model\n",
    "max_length = 512  # Increased for longer sequences\n",
    "lr = 5e-3  # Slightly lower learning rate for stability\n",
    "num_epochs = 1  # Reduced epochs for full dataset\n",
    "batch_size = 1  # Reduced batch size due to larger model\n",
    "gradient_accumulation_steps = 8  # Accumulate gradients to simulate larger batch\n",
    "eval_steps = 500  # Evaluate every 500 steps\n",
    "save_steps = 1000  # Save checkpoints every 1000 steps\n",
    "logging_steps = 50  # Log more frequently\n",
    "max_grad_norm = 1.0  # Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b2145-8b32-47d8-8e60-45905bcb277c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf47f06f-d661-433f-9017-9d1587857bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "    num_rows: 10178\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..'))) \n",
    "from src.helper_functions import format_letter_finetuning\n",
    "\n",
    "usml_raw = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "usml_train = usml_raw['train']\n",
    "print(usml_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf0231-7852-40ab-94c8-51f0aa72ce1a",
   "metadata": {},
   "source": [
    "## Pre-process the Dataset (Letter Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fdaa3-2560-4018-a5be-1541b6fb2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_train = usml_train.map(\n",
    "    format_letter_finetuning,\n",
    "    remove_columns=usml_train.column_names\n",
    ")\n",
    "\n",
    "print(formatted_train[0]['prompt'])\n",
    "print(formatted_train[0]['completion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13e7e1-6438-4e4f-89a1-e834106fad92",
   "metadata": {},
   "source": [
    "## Create train/validation split for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934f92a1-a13c-4296-88bf-6ec1f85d6292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 9669\n",
      "Validation samples: 509\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.95 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_dataset = full_train.select(range(train_size))\n",
    "val_dataset = full_train.select(range(train_size, train_size + val_size))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Format Data\n",
    "formatted_train = train_dataset.map(\n",
    "    format_mcf_finetuning,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "formatted_val = val_dataset.map(\n",
    "    format_mcf_finetuning,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5265c-e1e0-4661-8c30-9c917efada5f",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c36b67b-766f-4eb1-a240-f284f5b80055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4f2a0e96004331a1521fdeca42b899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82e29caed6849d78b1b5e2c267007ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Tokenization Function ---\n",
    "def tokenize_function(examples):\n",
    "    texts = [p + c for p, c in zip(examples['prompt'], examples['completion'])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels (mask prompt tokens)\n",
    "    prompt_lens = [len(tokenizer(p)['input_ids']) for p in examples['prompt']]\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i, plen in enumerate(prompt_lens):\n",
    "        labels[i, :plen] = -100\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# --- Apply tokenization ---\n",
    "print(\"Tokenizing training data...\")\n",
    "tokenized_train = formatted_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion'],\n",
    "    batch_size=8,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation data...\")\n",
    "tokenized_val = formatted_val.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion'],\n",
    "    batch_size=8,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739298e-a173-4f5f-b1b8-1c9e882f36ab",
   "metadata": {},
   "source": [
    "## Prefix Tuning Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5d2ef7-0604-4f86-9b70-8227d0e039cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama 3 8b model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeee8800b1a438084e62faac3b95a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 8,032,227,328 || trainable%: 0.0245\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Adjust prefix tuning config for Llama 3\n",
    "peft_config = PrefixTuningConfig(\n",
    "    peft_type=\"PREFIX_TUNING\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=30,\n",
    "    prefix_projection=False,\n",
    ")\n",
    "    \n",
    "# Load Llama 3 8b with optimized settings\n",
    "print(\"Loading Llama 3 8b model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=True,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c3f50-e6cc-43b7-af21-ab75ab6c2fd8",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ba94407-8281-40e2-b782-267d84ea8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-8b-usmle-prefix-letters-v1\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45e798-11a7-4943-87af-888d0783fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 1:36:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.761300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.651800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.603600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.584000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1209, training_loss=2.715461001305466, metrics={'train_runtime': 5778.5979, 'train_samples_per_second': 1.673, 'train_steps_per_second': 0.209, 'total_flos': 2.229200383597609e+17, 'train_loss': 2.715461001305466, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c8700-9064-4273-94b6-04f17441ea36",
   "metadata": {},
   "source": [
    "## Save adapters - to not exclude full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f34f48fc-a20e-44a8-b643-d5e6f58ff44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    \"llama3-8b-usmle-prefix-letters\",\n",
    "    safe_serialization=True,  # Uses modern .safetensors format\n",
    "    max_shard_size=\"200MB\"  # Optional: splits large adapters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "886e2fec-6b79-4cff-a16c-7064b40e4eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:1221: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-68851298-48383b7202a3bb5f72e5c761;2496cdfa-51ee-45e9-8796-24bad46fde79)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Meta-Llama-3-8B has been rejected by the repo's authors. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:238: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba5dc01b5104a59aa01b39068ca7f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/7.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pippalap/llama3-8b-usmle-prefix-letters/commit/4f2b62b6fc452b297c3ecee4ee582edfdd78b5f1', commit_message='Upload model', commit_description='', oid='4f2b62b6fc452b297c3ecee4ee582edfdd78b5f1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pippalap/llama3-8b-usmle-prefix-letters', endpoint='https://huggingface.co', repo_type='model', repo_id='pippalap/llama3-8b-usmle-prefix-letters'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define your custom model name\n",
    "MODEL_NAME = \"llama3-8b-usmle-prefix-letters\"  \n",
    "USERNAME = \"pippalap\"  # Your Hugging Face username\n",
    "\n",
    "model.push_to_hub(\"pippalap/llama3-8b-usmle-prefix-letters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3003f-89e5-4f90-8681-b8f4d54047f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
