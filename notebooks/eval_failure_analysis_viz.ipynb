{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cca91f1-44d0-4d5a-8e93-eff740a96783",
   "metadata": {},
   "source": [
    "# Visualizations for evaluation and failure analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c847d-f970-4f9c-b4fd-25177be11d79",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6d61c1-f77c-4497-8629-2c0bd0198fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import io\n",
    "\n",
    "GCS_BUCKET_NAME = \"open-llm-finetuning\"\n",
    "GCS_EVAL_PATH = f\"gs://{GCS_BUCKET_NAME}/data/evaluation\"\n",
    "GCS_OUTPUT_PATH = f\"gs://{GCS_BUCKET_NAME}/outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26650fb7-2dbb-4454-b990-a48f89e15620",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "\n",
    "Here we define functions to read the .json files containing the evaluation results from Google Cloud storage, we then process the data to export the visualization plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe3f711-03f1-4ef8-b7a0-2e7697aee5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data_from_gcs_directory(gcs_directory_path):\n",
    "    all_records = []\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name = gcs_directory_path.split('/')[2]\n",
    "        prefix = '/'.join(gcs_directory_path.split('/')[3:]) + '/'\n",
    "        \n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('.json'):\n",
    "                print(f\"Reading {blob.name}...\")\n",
    "                json_data = blob.download_as_text()\n",
    "                data = json.loads(json_data)\n",
    "\n",
    "                config_general = data['config_general']\n",
    "                results = data['results']\n",
    "                \n",
    "                for result_title, metrics in results.items():\n",
    "                    if result_title == 'all':\n",
    "                        continue\n",
    "                    \n",
    "                    accuracy = metrics.get('qem') or metrics.get('acc_norm') or metrics.get('acc')\n",
    "\n",
    "                    all_records.append({\n",
    "                        'model_name': config_general['model_name'],\n",
    "                        'accuracy': accuracy,\n",
    "                        'total_seconds': float(config_general['total_evaluation_time_secondes']),\n",
    "                        'source_file': os.path.basename(blob.name)\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing GCS. Details: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "def save_plot_to_gcs(fig, gcs_output_path, filename):\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = gcs_output_path.split('/')[2]\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "        \n",
    "    img_data = io.BytesIO()\n",
    "    fig.savefig(img_data, format='png', bbox_inches='tight')\n",
    "    img_data.seek(0)\n",
    "        \n",
    "    blob_path = os.path.join('/'.join(gcs_output_path.split('/')[3:]), filename)\n",
    "    blob = bucket.blob(blob_path)\n",
    "        \n",
    "    blob.upload_from_file(img_data, content_type='image/png')\n",
    "\n",
    "def generate_time_analysis_plots(df, gcs_output_path):\n",
    "    df_avg = df.groupby('model_name').agg({'total_seconds': 'mean'}).reset_index()\n",
    "    df_avg['short_model_name'] = df_avg['model_name'].apply(lambda x: x.split('/')[-1].upper())\n",
    "    df_avg['total_minutes'] = df_avg['total_seconds'] / 60\n",
    "    \n",
    "    df_with_letters = df_avg[df_avg['short_model_name'].str.contains(\"LETTERS\")]\n",
    "    df_without_letters = df_avg[~df_avg['short_model_name'].str.contains(\"LETTERS\")]\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    \n",
    "    sns.barplot(x='total_minutes', y='short_model_name', data=df_with_letters.sort_values('total_minutes', ascending=False), palette='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Generative Evaluation Time', fontsize=12, weight='bold')\n",
    "\n",
    "    sns.barplot(x='total_minutes', y='short_model_name', data=df_without_letters.sort_values('total_minutes', ascending=False), palette='viridis', ax=axes[1])\n",
    "    axes[1].set_title('Log-Likelihood Evaluation Time', fontsize=12, weight='bold')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Time (Minutes)', fontsize=12)\n",
    "        ax.set_ylabel('Model', fontsize=12)\n",
    "        for p in ax.patches:\n",
    "            ax.text(p.get_width() + 0.1, p.get_y() + p.get_height() / 2, f' {p.get_width():.1f} min', va='center', ha='left')\n",
    "\n",
    "    plt.suptitle('Average Evaluation Time per Model Type', fontsize=20, weight='bold')\n",
    "    \n",
    "    save_plot_to_gcs(fig, gcs_output_path, 'average_evaluation_time_combined.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def generate_comparison_plots(df, gcs_output_path):\n",
    "    def get_accuracy_from_df(filename, dataframe):\n",
    "        search_name = os.path.basename(filename)\n",
    "        return dataframe[dataframe['source_file'] == search_name]['accuracy'].iloc[0]\n",
    "\n",
    "    def create_plot(title, data_files, dataframe, output_filename, palette_name):\n",
    "        labels = list(data_files.keys())\n",
    "        palette = sns.color_palette(palette_name, 2)\n",
    "        \n",
    "        baseline_scores = [get_accuracy_from_df(data_files[cat]['Baseline'], dataframe) for cat in labels]\n",
    "        qlora_scores = [get_accuracy_from_df(data_files[cat]['QLORA'], dataframe) for cat in labels]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "        \n",
    "        rects1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline (Llama-3-8B)', color=palette[0])\n",
    "        rects2 = ax.bar(x + width/2, qlora_scores, width, label='Fine-tuned (QLORA)', color=palette[1])\n",
    "        \n",
    "        ax.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax.set_title(title, fontsize=16, weight='bold', pad=20)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, fontsize=12)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.set_ylim(0, max(1.0, max(baseline_scores + qlora_scores) * 1.2))\n",
    "\n",
    "        ax.bar_label(rects1, padding=3, fmt='%.4f')\n",
    "        ax.bar_label(rects2, padding=3, fmt='%.4f')\n",
    "        \n",
    "        save_plot_to_gcs(fig, gcs_output_path, output_filename)\n",
    "        plt.close(fig)\n",
    "\n",
    "    log_likelihood_files = {\n",
    "        'Cloze Formulation': {\n",
    "            'Baseline': 'evaluation_results_results_meta-llama_Meta-Llama-3-8B_results_2025-07-21T07-39-28.418935.json',\n",
    "            'QLORA': 'evaluation_results_results_jihbr_usmle-llama8b-qlora_results_2025-07-16T07-37-57.051004.json'\n",
    "        },\n",
    "        'Multiple Choice Formulation': {\n",
    "            'Baseline': 'evaluation_results_results_meta-llama_Meta-Llama-3-8B_results_2025-07-21T07-18-22.495821.json',\n",
    "            'QLORA': 'evaluation_results_results_jihbr_usmle-llama8b-qlora_results_2025-07-16T13-38-59.159732.json'\n",
    "        }\n",
    "    }\n",
    "    create_plot('Log-Likelihood Evaluation: Baseline vs. QLORA', log_likelihood_files, df, 'log_likelihood_comparison.png', 'viridis')\n",
    "\n",
    "    generative_files = {\n",
    "        'Letter Completion': {\n",
    "            'Baseline': 'evaluation_results_results_meta-llama_Meta-Llama-3-8B_results_2025-07-19T19-30-24.638550.json',\n",
    "            'QLORA': 'evaluation_results_results_jihbr_usmle-llama8b-qlora_results_2025-07-19T19-24-00.164953.json'\n",
    "        },\n",
    "        'Answer Completion': {\n",
    "            'Baseline': 'evaluation_results_results_meta-llama_Meta-Llama-3-8B_results_2025-07-19T20-04-07.914896.json',\n",
    "            'QLORA': 'evaluation_results_results_jihbr_usmle-llama8b-qlora_results_2025-07-19T19-52-24.700413.json'\n",
    "        }\n",
    "    }\n",
    "    create_plot('Generative Evaluation: Baseline vs. QLORA', generative_files, df, 'generative_evaluation_comparison.png', 'viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243275b-20b5-4d5c-8a11-041f8fe699d9",
   "metadata": {},
   "source": [
    "### Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306697f2-6e52-4de4-9d64-044ded17730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_6693/383749781.py:62: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='total_minutes', y='short_model_name', data=df_with_letters.sort_values('total_minutes', ascending=False), palette='viridis', ax=axes[0])\n",
      "/var/tmp/ipykernel_6693/383749781.py:65: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='total_minutes', y='short_model_name', data=df_without_letters.sort_values('total_minutes', ascending=False), palette='viridis', ax=axes[1])\n"
     ]
    }
   ],
   "source": [
    "generate_time_analysis_plots(master_df, gcs_output_path=GCS_OUTPUT_PATH)\n",
    "generate_comparison_plots(master_df, gcs_output_path=GCS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0c274-377b-433e-924c-25f3007e0419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
